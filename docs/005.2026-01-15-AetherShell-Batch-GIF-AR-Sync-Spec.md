# AetherShell Developer Log 005

## Project AetherShell: Frame-Sequence & Coordinate Analysis Spec

**Version:** 2.0 (Finalized Pipeline)  
**Date:** 2026-01-15  
**Subject:** Heterogeneous Integration of Legacy Stylus Input and AR Spatial Audio

---

## 1. Executive Summary

This specification defines the "Aether Sketch" workflow. The system bypasses traditional 3D modeling by utilizing **2D Texture Swapping** in 3D space. The "Intelligence" layer uses **Coordinate Pattern Recognition** (sending pixel locations as text to an LLM) to determine spatial audio triggers without the cost or latency of Vision-based AI.

---

## 2. Component Architecture & Lifecycle

### A. Phase 1: The Tactical Input (Nintendo DS)

- **Hardware:** Resistive Touchscreen, 256x192 1-bit canvas.
- **Workflow:** User draws 3 frames. Upon "DONE," the DS captures the VRAM buffer.
- **Transmission:** A single **HTTP POST** (Multipart) sends three raw bit-arrays to the Java Hub.
- **Tech Constraint:** No SSL/HTTPS. Data is sent to the local IP of the Hub.

### B. Phase 2: The Logic Engine (Python / Flask + LangChain)

Java receives the bytes and immediately hands them to Python for processing:

1. **Background Stripping:** Python iterates through the 1-bit map. `0` becomes `RGBA(0,0,0,0)` (Transparent) and `1` becomes `RGBA(0,0,0,255)` (Solid Black).
2. **Coordinate Extraction:** Python generates a "Sparsity Map." It converts every Nth black pixel into a string of $(x,y)$ coordinates.
3. **The LLM Pattern-Match (LangChain):**
   - **Input:** String of coordinates.
   - **Prompt:** "Analyze these points on a 256x192 grid. Identify the likely form: A) Animal/Low-center, B) Humanoid/Vertical, C) Inanimate/Geometric. Return only the letter."
4. **Return:** Python returns three transparent PNGs and the Classification ID to Java.

### C. Phase 3: The Persistence Layer (Java / Spring Boot + Postgres)

- **Storage:** Java saves the PNGs to a static file server and logs the `classification_id` in the `sketches` table.
- **Mapping:** Java maps `classification_id` to a specific `wav_file` (e.g., A = `meow.wav`, B = `hello.wav`).

### D. Phase 4: The Reality Viewport (iPhone / ARKit)

- **Animation:** The iPhone downloads the 3-image array. It initializes an `SCNPlane` and cycles the images every 200ms using an `SCNAction.repeatForever`.
- **Spatial Audio:**
  - The app creates an `SCNAudioSource` at the plane's 3D coordinates.
  - **Proximity Logic:** Uses a distance check $d = \sqrt{(x_2-x_1)^2 + (y_2-y_1)^2 + (z_2-z_1)^2}$.
  - If $d < 0.6$ meters, the audio triggers. Spatial mixing ensures the "Hello" or "Meow" comes from the exact 3D location of the drawing.

---

## 3. Detailed Data Flow

| Step  | Operation                 | Protocol  | Payload Type                     |
| :---- | :------------------------ | :-------- | :------------------------------- |
| **1** | DS $\rightarrow$ Java     | HTTP POST | Multipart (3x Raw Bit-Arrays)    |
| **2** | Java $\rightarrow$ Python | HTTP POST | JSON (Base64 Byte Arrays)        |
| **3** | Python $\rightarrow$ LLM  | HTTPS     | Text (Coordinate String)         |
| **4** | Python $\rightarrow$ Java | HTTP RESP | JSON (Paths + Class ID)          |
| **5** | Java $\rightarrow$ DB     | SQL       | `INSERT` (Asset Paths, Sound ID) |
| **6** | iOS $\rightarrow$ Java    | HTTPS GET | JSON (Metadata + Asset Links)    |

---

## 4. Technical Constraints & Mitigations

- **Coordinate Sparsity:** Sending every single pixel $(x,y)$ would exceed LLM token limits. Python will implement a "Sparsity Filter" (e.g., every 4th pixel) to maintain shape while reducing string length.
- **No-LiDAR Depth:** iPhone uses ARKit Plane Detection. The user must move the phone to "surface scan" before placing the sketch.
- **Transparency:** Python will output 32-bit PNGs to ensure the iPhone handles Alpha channels natively without "white box" artifacts.

---

_End of Specification_
